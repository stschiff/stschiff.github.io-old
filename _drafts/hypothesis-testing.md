---

Hypothesis testing without Alternatives
Significance tests doneÂ right
So, I am reading this book called â€œProbability Theory; The logic of Scienceâ€ by E.T. Jaynes. It is an excellent read, highly recommended, and quite an eye-opener to me in terms of understanding statistics and data analysis from first principles. Here I am sharing a few particularly interesting insights from chapter 9.11 (â€œSignificance Testsâ€).
A simple experiment
A p-value gives, in deliberately vague terms, something like the probability for some â€œNull-Hypothesisâ€ to be true. For example, letâ€™s say we perform an experiment of 100 repeated coin tosses, yielding 55 heads and 45 tails. As â€œNull-hypothesisâ€, we denote the hypothesis that the coin was â€œfairâ€ and that the experiment was performed in a fair manner, with the same expected frequency of 50% for both heads and tails (As a side remark, note that â€œfairâ€ in these assumptions is actually very difficult to define without circularity, which is another topic covered in the book). We can then compute a p-value for the Null-hypothesis given the observed data, which in this case turns out to be (Binomial tail-test) p=0.37. So given the usual â€œsignificance thresholdâ€ of 0.05, we would not reject the Null hypothesis and conclude that the results of the experiment are consistent with a fair coin and fair tossing.
OK, so far so good.
From a Bayesian perspective, there is a problem here. You cannot really talk about the â€œprobability for a hypothesisâ€ without defining among which alternative hypotheses you consider the Null-hypothesis. Somehow magically, the Binomial tail-test used above gets away with this flaw and still produces a (generally usable) p-value that means something. But what does it mean?


---

Let us denote the Null hypothesis by Hâ° and the alternative hypothesis by HÂ¹. Giving both of them prior probability 1/2, Bayesâ€™ theorem for Hâ° then reads
(where D denotes the data). For Hâ° we know what the likelihood of the data should be, it should be binomial. Specifically, for 55 heads out of 100 trials, and a heads-probabality of 50% under Hâ°, we have the binomial probability P(D|Hâ°)â‰ˆ0.048.
So, what type of alternative hypothesis should we consider? If you are tempted to test all possible alternatives, you run into problems. For example, it is always possible to cook up some pathological hypothesis that Jaynes calls â€œthe sure-thing hypothesisâ€, which has the property that P(D|HÂ¹)=1. This hypothesis states that the observed data is exactly produced with likelihood 1, and there could not have been any other result. Such a hypothesis means that not only the correct frequencies, but also the correct order of coin tosses is predicted. Clearly, we do not want to consider this hypothesis.
Instead, we would like to compare our Null-hypothesis with some set of reasonable alternative hypotheses. Specifically, the hypotheses that we would like to consider are all from the â€œBernoulli-classâ€ of hypothesis, which are parameterized by a single number ğœ†. These hypotheses state that i) all trials are independent, ii) at each trial, the probability for heads is ğœ†.Â 
To keep things simple, we would like to still make pairwise comparisons and avoid considering all alternative hypothesis at the same time (more on that later). It would therefore be instructive to consider the best possible Bernoulli-class hypothesis as our hypothesis HÂ¹. It is easy to show, and intuitively clear, that the best possible Bernoulli-class hypothesis for our data is the one where ğœ† equals the observed frequency of heads in the sample. Therefore, we consider ğœ†=0.55, which leads to P(D|HÂ¹)â‰ˆ0.08, giving posterior probability of P(Hâ°|D)â‰ˆ0.38, which is very close to the p-value obtained using the Binomial tail-test above.
So to conclude up to this point: In this simple setting, the p-value obtained from an orthodox significance test is similar to the posterior probability for the Null hypothesis compared to an implied best alternative hypothesis, which is simply the hypothesis that success probabilities equal observed success-frequencies.
So up to here, our Bayesian approach can let us feel a bit smug that we at least have a crystal-clear interpretation of our result (as a posterior probability) in contrast to the more vaguely defined p-values, but there is not much of a practical difference between the two approaches. This poses the question whether orthodox significance tests are always more or less doing the â€œrightâ€ thing. No, they donâ€™t, as the following example shows.
A more complicated experiment
Psi vs. Chi-squared
Hypothesis testing vs. parameter estimation